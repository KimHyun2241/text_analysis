{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  \n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from hanspell import spell_checker\n",
    "from json.decoder import JSONDecodeError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웹페이지로 리포트 보기\n",
    "# pr=data.profile_report() # 프로파일링 결과 리포트를 pr에 저장\n",
    "# data.profile_report() # 바로 결과 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일이 있는 상위 폴더 경로\n",
    "folder_path = \"data\"\n",
    "\n",
    "# 상위 폴더 내의 모든 파일 목록 가져오기\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# CSV 파일 목록만 추려내기\n",
    "csv_files = [file_name for file_name in file_names if file_name.endswith(\".csv\")]\n",
    "\n",
    "# 데이터프레임을 저장할 리스트 초기화\n",
    "df_list = []\n",
    "\n",
    "# CSV 파일 불러와서 데이터프레임 생성 후 리스트에 추가\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path, sep=\"|\", quoting=3, encoding=\"utf-8\")\n",
    "    df_list.append(df)\n",
    "\n",
    "# 모든 데이터프레임을 하나로 병합\n",
    "merged_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# 병합한 데이터프레임을 하나의 CSV 파일로 저장\n",
    "merged_df.to_csv(\"merged_data.csv\", index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"모든 CSV 파일을 병합하여 merged_data.csv 파일로 저장\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 불러오기\n",
    "csv_file_path = \"C:/python/language_analysis/merged_data.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file_path, sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('모든 데이터프레임의 길이 합:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열의 중복 제거\n",
    "df.drop_duplicates(subset=['reviews'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('모든 데이터프레임의 길이 합:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰에 포함된 줄 바꿈 html 제거\n",
    "df['reviews'] = df['reviews'].str.replace('<br/>', ' ')\n",
    "df['reviews'] = df['reviews'].str.replace('.', '')\n",
    "\n",
    "# 점수에 포함된 html 제거\n",
    "df['star'] = df['star'].str.replace('<div class=\"num\">', '')\n",
    "df['star'] = df['star'].str.replace('</div>', '')\n",
    "\n",
    "# 정규표현식 한글만 남김\n",
    "df['reviews'] = df['reviews'].apply(lambda x: re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null 값이 존재하는지 확인\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null 값이 존재하는 행 제거\n",
    "df = df.dropna(how = 'any')\n",
    "\n",
    "# Null 값이 존재하는지 확인\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null 값이 존재하는지 확인\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별점이 숫자형이 아니기 때문에 실수형으로 변환\n",
    "df['star'] = df['star'].astype(float)\n",
    "\n",
    "# 7.6 같은 별점의 경우 정수형으로 변환 시 7로 변환\n",
    "df['star'] = df['star'].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식과 간단한 전처리를 끝낸 후 새롭게 저장\n",
    "df.to_csv(\"new_merged_data.csv\", index=False, sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 불러오기\n",
    "csv_file_path = \"C:/python/language_analysis/new_merged_data.csv\"\n",
    "\n",
    "new_df = pd.read_csv(csv_file_path, sep=\"|\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 CSV 파일 경로\n",
    "csv_file_path = \"C:/python/language_analysis/new_merged_data.csv\"\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv(csv_file_path, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "# 데이터프레임을 10만 행씩 나누어 리스트에 저장\n",
    "dfs_to_save = [df[i:i+100000] for i in range(0, len(df), 100000)]\n",
    "\n",
    "# 10만 행씩 나눈 데이터프레임을 CSV 파일로 저장\n",
    "# 데이터가 너무 커서 나눠서 실행 하기 위함\n",
    "for i, df_chunk in enumerate(dfs_to_save):\n",
    "    df_chunk.to_csv(f\"chunk_{i+1}.csv\", sep=\"|\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# 파일 경로를 생성하는데 사용할 패턴 정의\n",
    "file_pattern = \"C:/python/language_analysis/chunk_{}.csv\"\n",
    "\n",
    "# 1부터 4까지의 파일을 읽어와서 리스트에 추가\n",
    "for i in range(1, 5):\n",
    "    file_path = file_pattern.format(i)\n",
    "    df = pd.read_csv(file_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# dfs 리스트에 있는 모든 데이터프레임을 순서대로 df1, df2, df3, df4에 할당\n",
    "df1, df2, df3, df4 = dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러가 발생한 인덱스를 저장할 리스트\n",
    "error_indices = []\n",
    "\n",
    "# 몇 번 까지 실행이 완료 했는지 보기 위함\n",
    "aa = 0\n",
    "# 맞춤법 검사\n",
    "\n",
    "# 데이터 크기가 커서 df1 ~ df4로 나눠서 작업 실행\n",
    "# 총 소요시간 630분 -> 약 12시간 30분\n",
    "for i in range(len(df4)):\n",
    "    try:\n",
    "        a = df4['reviews'][i]\n",
    "        spelled_sent = spell_checker.check(a)\n",
    "        hanspell_sent = spelled_sent.checked\n",
    "        df4['reviews'][i] = hanspell_sent\n",
    "\n",
    "        # i가 5000의 배수일 때 10분 동안 대기\n",
    "        if i % 5000 == 0 and i > 0:\n",
    "            print(f\"Pausing for 10 minutes at index {i}\")\n",
    "            time.sleep(10)\n",
    "            \n",
    "    except JSONDecodeError as e:\n",
    "        error_indices.append(i)  # 에러가 발생한 인덱스를 저장\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred at index {i}: {e}\")\n",
    "\n",
    "    # 몇 번 까지 실행이 완료 했는지 보기 위함\n",
    "    aa+=1\n",
    "    print(aa)\n",
    "\n",
    "print(\"Completed with errors at indices:\", error_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가공이 끝난 데이터프레임을 finish_chunk_1.csv 파일로 저장\n",
    "df1.to_csv(\"finish_chunk_1.csv\", sep=\"|\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가공이 끝난 데이터프레임을 finish_chunk_1.csv 파일로 저장\n",
    "df2.to_csv(\"finish_chunk_2.csv\", sep=\"|\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가공이 끝난 데이터프레임을 finish_chunk_1.csv 파일로 저장\n",
    "df3.to_csv(\"finish_chunk_3.csv\", sep=\"|\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가공이 끝난 데이터프레임을 finish_chunk_1.csv 파일로 저장\n",
    "df4.to_csv(\"finish_chunk_4.csv\", sep=\"|\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):    \n",
    "    # 파일 저장하기\n",
    "    df.to_csv(\"finish_chunk_\".format(i), sep=\"|\", encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일이 있는 상위 폴더 경로\n",
    "folder_path = \"finish_data\"\n",
    "\n",
    "# 상위 폴더 내의 모든 파일 목록 가져오기\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# CSV 파일 목록만 추려내기\n",
    "csv_files = [file_name for file_name in file_names if file_name.endswith(\".csv\")]\n",
    "\n",
    "# 데이터프레임을 저장할 리스트 초기화\n",
    "df_list = []\n",
    "\n",
    "# CSV 파일 불러와서 데이터프레임 생성 후 리스트에 추가\n",
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    df = pd.read_csv(file_path, sep=\"|\", quoting=3, encoding=\"utf-8\")\n",
    "    df_list.append(df)\n",
    "\n",
    "# 모든 데이터프레임을 하나로 병합\n",
    "merged_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# 병합한 데이터프레임을 하나의 CSV 파일로 저장\n",
    "merged_df.to_csv(\"merged_finish_data.csv\", index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "\n",
    "# 결과 출력\n",
    "print(\"모든 CSV 파일을 병합하여 merged_finish_data.csv 파일로 저장\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_finish_data.csv\", sep=\"|\", quoting=3, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null 값이 존재하는지 확인\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null 값이 존재하는 행 제거\n",
    "df = df.dropna(how = 'any')\n",
    "\n",
    "# Null 값이 존재하는지 확인\n",
    "print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null 값이 존재하는지 확인\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('데이터프레임의 길이:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['reviews'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df를 무작위로 섞기\n",
    "df_shuffled = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# train과 test에 7:3으로 나누기\n",
    "train_data, test_data = train_test_split(df_shuffled, test_size=0.3, random_state=42)\n",
    "\n",
    "# 각각을 CSV 파일로 저장\n",
    "train_data.to_csv('train_data.csv', index=False, sep=\"|\", encoding=\"utf-8\")\n",
    "test_data.to_csv('test_data.csv', index=False, sep=\"|\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '분들', '이다']\n",
    "# ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '분들', \"이다\"]\n",
    "'하다', '게', '분들', '는', '가', '를', '인', '과', '좀', '도', '에', '고', '지', '임', '네', '이다', '은', '들', '한', '나', '다', '의', '걍'\n",
    "\n",
    "okt = Okt()\n",
    " \n",
    "X_train = []\n",
    "for sentence in tqdm(train_data['review']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "# 데이터프레임 합친 것\n",
    "for sentence in tqdm(test_data['review']):\n",
    "    # 토큰화\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
    "    # 불용어 제거\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords]\n",
    "    # 한글에서 자음 또는 모음만으로 이루어진 문자 삭제\n",
    "    stopwords_removed_sentence = [word for word in stopwords_removed_sentence if not re.match(r'^[ㄱ-ㅎㅏ-ㅣ]+$', word)]\n",
    "    \n",
    "    X_test.append(stopwords_removed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터에 정수 인코딩을 수행해야 한다.\n",
    "# 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "\n",
    "# 단어의 수\n",
    "total_cnt = len(tokenizer.word_index)\n",
    "\n",
    "# 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "rare_cnt = 0\n",
    "\n",
    "# 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "total_freq = 0\n",
    "\n",
    "# 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "rare_freq = 0\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받기\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 토크나이저의 인자로 넘겨주고 텍스트 시퀀스를 정수 시퀀스로 변환.\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 작업\n",
    "print('리뷰의 최대 길이 :',max(len(review) for review in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 샘플 중 길이가 max_len 이하인 샘플의 비율이 몇 %인지 확인하는 함수\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  count = 0\n",
    "  for sentence in nested_list:\n",
    "    if(len(sentence) <= max_len):\n",
    "        count = count + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_len의 크기와 비율은 비례\n",
    "max_len = 60\n",
    "below_threshold_len(max_len, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 샘플의 길이 max_len으로 맞춤\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터인 임베딩 벡터의 차원 크기\n",
    "embedding_dim = 100\n",
    "# 은닉 상태의 크기\n",
    "hidden_units = 128\n",
    "# 모델 학습\n",
    "for _ in range(5):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    # 다 대 일 구조\n",
    "    # 마지막 시점에서 두 개의 선택지 중 하나를 예측하는 이진 분류 문제를 수행하는 모델\n",
    "    model.add(LSTM(hidden_units))\n",
    "    # 이진 분류 문제의 경우, 출력층에 로지스틱 회귀를 사용해야 하므로 활성화 함수로는 시그모이드 함수를 사용\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 검증 데이터 손실이 4회 증가하면\n",
    "    #  정해진 에포크가 도달하지 못하였더라도 학습을 조기 종료(Early Stopping)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    # 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델을 저장\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    # validation_split=0.2을 사용하여 훈련 데이터의 20%를 검증 데이터로 분리해서 사용하고,\n",
    "    # 검증 데이터를 통해서 훈련이 적절히 되고 있는지 확인\n",
    "    history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 모델 저장\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "     pickle.dump(tokenizer, handle)\n",
    "\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "  score = float(loaded_model.predict(pad_new)) # 예측\n",
    "  if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "  else:\n",
    "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"전체적으로 무난 하였으나 방 크기가 작아서 조금 아쉬웠습니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"방이 너무 더러워서 환불 요청했지만 받아주지않고 너무 별로네요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"다신 안감 서비스 최악\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"방 냄새남\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"와인잔이 대여라고 되어있어서 와인만 사서 갔는데..와인잔은 대여되지않고 판매입니다 참고하세요카운터에 문의했더니 다음날 지배인오면 알아보고 연락준다고 했는데 연락도 안옴.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"너무 오래되고 냄새 너무남\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"저렴하고 깨끗하여 전반적으로 만족합니다다만 방음이 잘 안되는 단점이 있어요저는 소음이 있어도 잘 잤지만 예민하신 분은 힘들 수도 있을 것 같아요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"더블 데이트 때 잘 이용하고 가요 너무 친절하고 좋았습니당\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"늦은퇴실로 예약했는데 갑자기 12시전에 전화오셔서 퇴실해달라고 했습니다, 잘못예약했나싶어 나오긴했지만  다시보니 늦은퇴실로 예약로 했었네요 ..돈 더 주고 예약한건데 그런 부주위한 점이 아쉽네요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"생각보다 너무 좁고 화장실 슬리퍼는 뜯어질려고 해서… 춥지는 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"친절하시지만 방에 담배 냄새는 어쩔수없네요...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"불친절하고 방에 담배 냄새는 어쩔수없네요...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"친절한건지 불친절한건지 모르겠고 방에 담배 냄새는 어쩔수없네요...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"방에 담배 냄새는 어쩔수없네요...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"수건도 더 챙겨주시고 물도 더 주셨어요 근데 빗에 머리카락이 그대로 뭉쳐져 있더라구요 그 부분은 조금 아쉬웠어요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"비오는날이라 그런건지 아님 원래 그런건지 테이블 청소도 너무 안돼있고,크로아상 옆쪽에 하루살이들이 엄청 많더라구요 ..... 위생이 아쉽네요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"연박인데..청소를 안해주시네요ㅜㅜ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"종이가 맞는건지 아니면 어플이 맞는건지.. 라운지 이용도 7시부터라고 적어두셨는데 예약을 해야 가능한거 같더라고요 사용 가능하냐 물어봤더니 개인적으로는 불가능하다 하시고 한 한 두시간뒤에 보니까 파티하고 계시더라구여 안에서 세탁은 자유롭게 이용 가능했고 수압이 되게 약해요ㅠ 그래도 뜨거운물은 잘 나옵니다 방음은 전혀 안되고 옆집 윗집 앞집 소리 다 나요...아침에 청소하실때 그 소리도 다 들림 드르륵 거리는 소리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"3박 4일 동안 너무 편안하고,, 깔끔하게 사용했어요^^ 아침 조식도 너무 든든하고 맛있었답니당 눈 뜨면 바로 광안리 바다가 눈 앞에 보인게 제일 좋았어용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"위치도 좋고 했지만 예전에 비해 숙소 청결도가 좀 낮아졌어요. 화장실에서 냄새도 나고 방충망도 뜯겨져있고\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predict(\"이상한 벌레가 물린거빼곤 좋았어요\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
